apiVersion: v1
kind: Secret
metadata:
  name: s3-log-keys
  namespace: ${KUBE_NAMESPACE}
type: Opaque
stringData:
  AWS_ACCESS_KEY_ID: "${SCW_LOG_ACCESS_KEY}"
  AWS_SECRET_ACCESS_KEY: "${SCW_LOG_SECRET_KEY}"
---
apiVersion: v1
kind: Secret
metadata:
  name: piste-api-keys
  namespace: ${KUBE_NAMESPACE}
type: Opaque
stringData:
  PISTE_JUDILIBRE_KEY: "${PISTE_JUDILIBRE_KEY}"
  PISTE_JUDILIBRE_KEY_PROD: "${PISTE_JUDILIBRE_KEY_PROD}"
  PISTE_METRICS_KEY: "${PISTE_METRICS_KEY}"
  PISTE_METRICS_SECRET: "${PISTE_METRICS_SECRET}"
  PISTE_METRICS_KEY_PROD: "${PISTE_METRICS_KEY_PROD}"
  PISTE_METRICS_SECRET_PROD: "${PISTE_METRICS_SECRET_PROD}"
---
apiVersion: v1
kind: Secret
metadata:
  name: ${APP_GROUP}-es-roles
  namespace: ${KUBE_NAMESPACE}
stringData:
  roles.yml: |-
    search:
      run_as: [ 'search' ]
      cluster:
      - monitor
      indices:
      - names: [ 'logstash-*' ]
        privileges: [ 'read' ]
---
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: monitor
  namespace: monitor
spec:
  version: ${ELASTIC_VERSION}
  volumeClaimDeletePolicy: ${ELASTIC_STORAGE_POLICY}
  auth:
    fileRealm:
    - secretName: ${APP_GROUP}-es-users
    roles:
    - secretName: ${APP_GROUP}-es-roles
  nodeSets:
  - name: default
    count: ${ELASTIC_NODES}
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: ${ELASTIC_STORAGE_SIZE}
    podTemplate:
      spec:
        initContainers:
        - name: sysctl
          securityContext:
            privileged: true
          command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']
        containers:
        - name: elasticsearch
          env:
          - name: ES_JAVA_OPTS
            value: -Xms${ELASTIC_MEM_JVM} -Xmx${ELASTIC_MEM_JVM}
          - name: LOG4J_FORMAT_MSG_NO_LOOKUPS
            value: "true"
          resources:
            requests:
              memory: ${ELASTIC_MEM}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-configmap
  namespace: monitor
data:
  logstash.yml: |
    http.host: "0.0.0.0"
    path.config: /usr/share/logstash/pipeline
    pipeline.ecs_compatibility: disabled
  logstash.conf: |
    input {
      file {
        type => "json"
        path => "/var/log/s3/**/*.jsonl"
        mode => "read"
        file_completed_action => "log"
        file_completed_log_path => "/usr/share/logstash/data/plugins/inputs/file/completed.log"
        discover_interval => 60
        stat_interval => 1
        codec => "json"
      }
    }
    filter {
      date {
        match => [ "date" , "ISO8601" ]
      }
      mutate {
        add_field => {
          "source" => "%{[path]}"
        }
      }
      grok {
        match => {
          "[source]" => ".*/(?<kubernetes_cluster_name>judilibre-scw-[^\/]*)-(master|dev)/.*$"
        }
        tag_on_failure => [ "kubernetes_cluster_name_error" ]
      }
      if "kubernetes_cluster_name_error" in [tags] {
        mutate {
          add_field => { "kubernetes_cluster_name" => "default" }
        }
      }
      mutate {
        rename => {
          "[kubernetes_labels][app]" => "[kubernetes_labels][app.kubernetes.io/name]"
        }
      }
      if [cpu_p] or [Mem.total] or [write_size] {
        mutate {
          add_field => { "log_type" => "metrics" }
        }
      } else {
        if [kubernetes_container_name] == "nginx-ingress-controller" or [kubernetes_namespace_name] =~ /^judilibre/ {
          grok {
            match => {
              "log" => "%{TIMESTAMP_ISO8601:container_timestamp} %{WORD:container_output} %{WORD:container_output_mode} %{GREEDYDATA:container_log}"
            }
            tag_on_failure => [ "kubernetes_log_parse_error" ]
          }
          if [container_output] {
              mutate {
                remove_field => ["log"]
              }
            if [container_output] == "stderr" {
              mutate {
                add_tag => ["container_error"]
              }
            }
            if [kubernetes_container_name] == "nginx-ingress-controller" {
              if [container_output] != "stderr" {
                mutate {
                  add_field => { "log_type" => "web_access" }
                }
                grok {
                  match => { "container_log" => "%{IPORHOST:clientip} (?:-|(%{WORD}.%{WORD})) (-|%{USER:http_user}) \[%{HTTPDATE:timestamp_request}\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" (-|%{NUMBER:status;long}) (-|%{NUMBER:body_bytes_sent;long}) (?:\"(?:%{URI:http_referrer}|-)\") \"%{DATA:http_user_agent}\" (-|%{NUMBER:request_length;long}) (-|%{NUMBER:request_time;double}) \[(?:%{DATA:proxy_upstream_name})\] \[(?:%{DATA:proxy_alternative_upstream_name})\] (-|%{NOTSPACE:upstream_addr}|(?<upstream_addr>(\d+.\d+.\d+.\d+:\d+(, )?|-)+)) (-|%{NUMBER:upstream_response_length;long}|(?<upstream_response_length>(\d+(, )?|-)+)) (-|%{NUMBER:upstream_response_time;double}|(?<upstream_response_time>([0-9\.]+(, )?|-)+)) (-|%{NUMBER:upstream_status;long}|(?<upstream_status>(\d+(, )?|-)+)) %{NOTSPACE:request_id}" }
                }
                if "_grokparsefailure" in [tags] {
                  mutate {
                    add_tag => [ "nginx_log_parse_error" ]
                  }
                } else {
                  mutate {
                    remove_field => ["container_log"]
                    convert => {
                      body_bytes_sent => integer
                      request_length => integer
                      request_time => float
                    }
                  }
                  grok {
                    match => [ "request", "%{URIPARAM:request_params}" ]
                  }
                  if [request_params] {
                    mutate {
                      gsub => [ "request_params", "(\?|\[\]|\/)", "" ]
                      gsub => [ "request_params", "\+", " " ]
                      lowercase => [ "request_params" ]
                    }
                    urldecode {
                      field => "request_params"
                    }
                    kv {
                      prefix => "request_params_"
                      source => "request_params"
                      field_split => "&"
                    }
                  }
                  geoip {
                    source => "clientip"
                  }
                  if [proxy_upstream_name] =~ /search/ {
                    grok {
                      match => {
                        "[request]" => "^/(?<request_api>(decision|search|taxonomy|stats|healthcheck)).*"
                      }
                      tag_on_failure => [ "request_invalid" ]
                    }
                  } else if [proxy_upstream_name] =~ /admin/ {
                    grok {
                      match => {
                        "[request]" => "^/(?<request_api>(admin|delete|import|index)).*"
                      }
                      tag_on_failure => [ "request_invalid" ]
                    }
                  }
                }
              } else {
                mutate {
                  add_field => { "log_type" => "web_error" }
                }
              }
            } else {
              if [kubernetes_container_name] == "nginx-ingress-controller" {
                mutate {
                  add_field => { "log_type" => "web_error" }
                }
              } else {
                mutate {
                  add_field => { "log_type" => "judilibre" }
                }
              }
            }
          } else {
            mutate {
              add_field => { "log_type" => "judilibre" }
            }
          }
        } else {
          drop {}
        }
      }
    }
    output {
      elasticsearch {
        index => "logstash-%{[kubernetes_cluster_name]}-%{[log_type]}-%{+YYYY.MM}"
        user => "elastic"
        password => "${ELASTIC_PASSWORD}"
        hosts => ["https://monitor-es-http:9200"]
        cacert => "/etc/logstash/certificates/ca.crt"
        ssl => true
        action => "create"
      }
    }
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: s3-log-mirror
  namespace: monitor
  labels:
    app: logstash
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: logstash-data-plugins
  namespace: monitor
  labels:
    app: logstash
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: s3-rclone-sync
  namespace: monitor
spec:
  schedule: "* * * * *"
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      backoffLimit: 0
      # activeDeadlineSeconds: 7200
      template:
        spec:
          containers:
          - name: rclone
            image: rclone/rclone:latest
            volumeMounts:
              - name: s3-log-mirror
                mountPath: /var/log/s3
            command:
            - /bin/sh
            args:
            - -c
            - |
              set -e;
              LOG_DIR=/var/log/s3/
              echo archives:${SCW_LOG_ARCHIVE_BUCKET} logs:${SCW_LOG_BUCKET}
              rclone -v copy --checkers=8 --transfers=8 --ignore-existing s3:${SCW_LOG_BUCKET} ${LOG_DIR};
              if [ "${SCW_LOG_GZIP}" == "true" ]; then
                for LOG_DIR_ENVDAY in $(find ${LOG_DIR} -mindepth 5 -maxdepth 5 -type d | grep -v "/$(date +%Y%m%d)" | grep -v "/$(date --date=yesterday +%Y%m%d)" | sort);do
                  echo concatenating and zipping ${LOG_DIR_ENVDAY};
                  (cat $(find ${LOG_DIR_ENVDAY} -type f -iname '*.jsonl' | sort) | gzip > ${LOG_DIR_ENVDAY}.jsonl.gz) && rm -rf ${LOG_DIR_ENVDAY};
                done
                rclone -v copy --checkers=8 --transfers=8 --ignore-existing ${LOG_DIR} s3:${SCW_LOG_ARCHIVE_BUCKET} --include=**.jsonl.gz;
                rclone -v delete --checkers=8 --transfers=8 s3:${SCW_LOG_BUCKET} --exclude=**/$(date +%Y%m%d)** --exclude=**/$(date --date=yesterday +%Y%m%d)**;
              fi;
            env:
            - name: RCLONE_CONFIG_S3_TYPE
              value: s3
            - name: RCLONE_CONFIG_S3_ENV_AUTH
              value: "false"
            - name: RCLONE_CONFIG_S3_ENDPOINT
              value: s3.${SCW_REGION}.scw.cloud
            - name: RCLONE_CONFIG_S3_REGION
              value: ${SCW_REGION}
            - name: RCLONE_CONFIG_S3_SERVER_SIDE_ENCRYPTION
              value: ""
            - name: RCLONE_CONFIG_S3_FORCE_PATH_STYLE
              value: "false"
            - name: RCLONE_CONFIG_S3_LOCATION_CONSTRAINT
              value: ""
            - name: RCLONE_CONFIG_S3_STORAGE_CLASS
              value: ""
            - name: RCLONE_CONFIG_S3_ACL
              value: private
            - name: RCLONE_CONFIG_S3_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-log-keys
                  key: AWS_ACCESS_KEY_ID
            - name: RCLONE_CONFIG_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-log-keys
                  key: AWS_SECRET_ACCESS_KEY
          restartPolicy: Never
          volumes:
          - name: s3-log-mirror
            persistentVolumeClaim:
              claimName: s3-log-mirror
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: logstash-deployment
  namespace: monitor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: logstash
  template:
    metadata:
      labels:
        app: logstash
    spec:
      initContainers:
      - name: fix-permissions
        image: busybox
        command:
        - sh
        - -c
        - |
          chown -R 1000 /usr/share/logstash/data/plugins
        volumeMounts:
        - name: logstash-data-plugins
          mountPath: /usr/share/logstash/data/plugins
      containers:
      - name: logstash
        image: docker.elastic.co/logstash/logstash:${ELASTIC_VERSION}
        ports:
        - containerPort: 5044
        env:
          - name: ELASTIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: ${APP_GROUP}-es-elastic-user
                key: elastic
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: s3-log-keys
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: s3-log-keys
                key: AWS_SECRET_ACCESS_KEY
        volumeMounts:
          - name: config-volume
            mountPath: /usr/share/logstash/config
          - name: logstash-pipeline-volume
            mountPath: /usr/share/logstash/pipeline
          - name: cert-ca
            mountPath: /etc/logstash/certificates
            readOnly: true
          - name: s3-log-mirror
            readOnly: true
            mountPath: /var/log/s3
          - name: logstash-data-plugins
            readOnly: false
            mountPath: /usr/share/logstash/data/plugins
      volumes:
      - name: config-volume
        configMap:
          name: logstash-configmap
          items:
            - key: logstash.yml
              path: logstash.yml
      - name: logstash-pipeline-volume
        configMap:
          name: logstash-configmap
          items:
            - key: logstash.conf
              path: logstash.conf
      - name: cert-ca
        secret:
          secretName: monitor-es-http-certs-public
      - name: s3-log-mirror
        persistentVolumeClaim:
          claimName: s3-log-mirror
      - name: logstash-data-plugins
        persistentVolumeClaim:
          claimName: logstash-data-plugins
---
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: monitor
  namespace: monitor
spec:
  version: ${ELASTIC_VERSION}
  count: 1
  elasticsearchRef:
    name: monitor
